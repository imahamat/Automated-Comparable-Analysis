df1 = read.csv("ListS&P500.csv", row.names = 1, sep = ';')
df = df1[-1]
# Omit non available values if there are any left (pre-treated dataset)
df <- na.omit(df)
# Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
df <- scale(df)
View(df1)
View(df)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(gridExtra)
library(fpc)
library(dbscan)
library(mclust)
library(adamethods)
library(car) # Companion to Applied Regression
library(caret)
library(randomForest)
library(mlbench)
library(fpc)
library(knitr) # package for building complex tables
library(kableExtra)
library(viridisLite) # color map package
# Chunk 3
# n is the number of outliers to remove
df1 = read.csv("ListS&P500.csv", row.names = 1, sep = ';')
df = df1[-1]
# Omit non available values if there are any left (pre-treated dataset)
df <- na.omit(df)
# Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
df <- scale(df)
# Chunk 4
df
# Chunk 5
# ensure the results are repeatable
#set.seed(7)
# calculate correlation matrix
#correlationMatrix <- cor(df)
# summarize the correlation matrix
#print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
#highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
#print(highlyCorrelated)
# Chunk 6
new.df = as.data.frame.matrix(df)
mod <- lm(new.df$Pre.Tax.ROE ~ ., data = new.df)
# Chunk 7
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]
#new.df[influential, ]
# Chunk 8
CompanyIndex = function(Df, vect)
{
indexes = 0
for (i in 1:length(vect))
{
indexes[i] = which(rownames(Df) == vect[i])
}
return (indexes)
}
# Chunk 9
outliers1 = CompanyIndex(Dataset(), influential)
outliers1
# Chunk 10
car_outliers = outlierTest(mod)
vect = names(car_outliers$rstudent)
outliers2 = CompanyIndex(df, vect)
car_outliers
outliers2
# Chunk 11
Outliers = function(Df, n, c)
{
#parameters : Df = dataframe, n = number of outliers, c = number of centers for clustering
PCA_Data = princomp(Df, cor = T)
PC1 = PCA_Data$scores[,1]
PC2 = PCA_Data$scores[,2]
X = cbind(PC1, PC2)
km = kmeans(X, centers = c)
centers = km$centers[km$cluster,]
distances = sqrt(rowSums((X-centers)^2))
outliers = order(distances, decreasing = T)[1:n]
plot(X, pch = 16, col = km$cluster, cex = 1, main = paste0("C = ", c, "     N = ", n))
points(km$centers, pch = 23, bg = "yellow", cex = 2, lwd = 2)
points(X[outliers,], pch = 25, col = "orange", cex = 2)
legend("topleft", legend = c("Cluster center", "Outliers"), pt.cex = 2, pch = c(23, 25), col = c("black", "orange"), pt.bg = c("yellow", NA))
return (outliers)
}
# Chunk 12
outliers3 = Outliers(df,30,1)
outliers3
# Chunk 13
outliers4 = do_knno(df, 10, 10)
outliers4
# Chunk 14
dataset = df[-outliers3,]
data = as.data.frame.matrix(dataset)
# Chunk 15
distance <- get_dist(dataset)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(dataset, centers = 2, nstart = 25)
p1 = fviz_cluster(k2, data = dataset) + ggtitle("k = 2")
k3 = kmeans(dataset, centers = 3, nstart = 25)
p2 = fviz_cluster(k3, data = dataset) + ggtitle("k = 3")
k4 = kmeans(dataset, centers = 4, nstart = 25)
p3 = fviz_cluster(k4, data = dataset) + ggtitle("k = 4")
grid.arrange(p1, p2, p3, nrow = 2)
# Chunk 16
set.seed(123)
fviz_nbclust(dataset, kmeans, method = "wss", k.max = 20)
fviz_nbclust(dataset, kmeans, method = "silhouette", k.max = 20)
fviz_nbclust(dataset, kmeans, method = "gap_stat", k.max = 20)
# Chunk 17
# Compute k-means
k = kmeans(dataset, centers = 7, nstart = 25)
a1 = fviz_cluster(k, data = dataset) + ggtitle("k-means")
# Compute PAM
pam <- pam(dataset, 7)
a2 = fviz_cluster(pam) + ggtitle("PAM")
# Compute CLARA
clara <- clara(dataset, 7, samples = 50, pamLike = TRUE)
a3 = fviz_cluster(clara) + ggtitle("CLARA")
grid.arrange(a1, a2, a3, nrow = 2)
PCA_Data = princomp(dataset, cor = T)
#str(PCA_Data)
#summary(PCA_Data)
PC1 <- PCA_Data$scores[,1]
PC2 <- PCA_Data$scores[,2]
X <- cbind(PC1, PC2)
km <- kmeans(X, centers = 7)
plot(PC1, PC2, col = km$cluster, xlab = "PC1", ylab = "PC2", main = "K-means clustering with PCA")
points(km$centers, col = 1:7, pch = 3, cex = 2, lwd = 3)
# Chunk 18
OrderClust = function(clust)
{
o = order(clust$cluster)
kmclust = data.frame(clust$cluster[o])
return (kmclust)
}
# Chunk 19
d1 = OrderClust(km)
d2 = OrderClust(pam)
d3 = OrderClust(clara)
result = cbind(d1, d2, d3)
# Chunk 20
set.seed(123)
res1 = data %>%
scale() %>%
eclust("kmeans", k = 7, graph = TRUE)
res2 = data %>%
scale() %>%
eclust("pam", k = 7, graph = TRUE)
res3 = data %>%
scale() %>%
eclust("clara", k = 7, graph = TRUE)
s1 = fviz_silhouette(res1)
s2 = fviz_silhouette(res2)
s3 = fviz_silhouette(res3)
grid.arrange(s1, s2, s3, nrow = 2)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(gridExtra)
library(fpc)
library(dbscan)
library(mclust)
library(adamethods)
library(car) # Companion to Applied Regression
library(caret)
library(randomForest)
library(mlbench)
library(fpc)
library(knitr) # package for building complex tables
library(kableExtra)
library(viridisLite) # color map package
# Chunk 3
# n is the number of outliers to remove
df1 = read.csv("ListS&P500.csv", row.names = 1, sep = ';')
df = df1[-1]
# Omit non available values if there are any left (pre-treated dataset)
df <- na.omit(df)
# Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
df <- scale(df)
# Chunk 4
# ensure the results are repeatable
#set.seed(7)
# calculate correlation matrix
#correlationMatrix <- cor(df)
# summarize the correlation matrix
#print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
#highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
#print(highlyCorrelated)
# Chunk 5
new.df = as.data.frame.matrix(df)
mod <- lm(new.df$Pre.Tax.ROE ~ ., data = new.df)
# Chunk 6
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]
#new.df[influential, ]
# Chunk 7
CompanyIndex = function(Df, vect)
{
indexes = 0
for (i in 1:length(vect))
{
indexes[i] = which(rownames(Df) == vect[i])
}
return (indexes)
}
# Chunk 8
outliers1 = CompanyIndex(Dataset(), influential)
outliers1
# Chunk 9
car_outliers = outlierTest(mod)
vect = names(car_outliers$rstudent)
outliers2 = CompanyIndex(df, vect)
car_outliers
outliers2
# Chunk 10
Outliers = function(Df, n, c)
{
#parameters : Df = dataframe, n = number of outliers, c = number of centers for clustering
PCA_Data = princomp(Df, cor = T)
PC1 = PCA_Data$scores[,1]
PC2 = PCA_Data$scores[,2]
X = cbind(PC1, PC2)
km = kmeans(X, centers = c)
centers = km$centers[km$cluster,]
distances = sqrt(rowSums((X-centers)^2))
outliers = order(distances, decreasing = T)[1:n]
plot(X, pch = 16, col = km$cluster, cex = 1, main = paste0("C = ", c, "     N = ", n))
points(km$centers, pch = 23, bg = "yellow", cex = 2, lwd = 2)
points(X[outliers,], pch = 25, col = "orange", cex = 2)
legend("topleft", legend = c("Cluster center", "Outliers"), pt.cex = 2, pch = c(23, 25), col = c("black", "orange"), pt.bg = c("yellow", NA))
return (outliers)
}
# Chunk 11
outliers3 = Outliers(df,30,1)
outliers3
# Chunk 12
outliers4 = do_knno(df, 10, 10)
outliers4
# Chunk 13
dataset = df[-outliers3,]
data = as.data.frame.matrix(dataset)
# Chunk 14
distance <- get_dist(dataset)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(dataset, centers = 2, nstart = 25)
p1 = fviz_cluster(k2, data = dataset) + ggtitle("k = 2")
k3 = kmeans(dataset, centers = 3, nstart = 25)
p2 = fviz_cluster(k3, data = dataset) + ggtitle("k = 3")
k4 = kmeans(dataset, centers = 4, nstart = 25)
p3 = fviz_cluster(k4, data = dataset) + ggtitle("k = 4")
grid.arrange(p1, p2, p3, nrow = 2)
# Chunk 15
set.seed(123)
fviz_nbclust(dataset, kmeans, method = "wss", k.max = 20)
fviz_nbclust(dataset, kmeans, method = "silhouette", k.max = 20)
fviz_nbclust(dataset, kmeans, method = "gap_stat", k.max = 20)
# Chunk 16
# Compute k-means
k = kmeans(dataset, centers = 7, nstart = 25)
a1 = fviz_cluster(k, data = dataset) + ggtitle("k-means")
# Compute PAM
pam <- pam(dataset, 7)
a2 = fviz_cluster(pam) + ggtitle("PAM")
# Compute CLARA
clara <- clara(dataset, 7, samples = 50, pamLike = TRUE)
a3 = fviz_cluster(clara) + ggtitle("CLARA")
grid.arrange(a1, a2, a3, nrow = 2)
PCA_Data = princomp(dataset, cor = T)
#str(PCA_Data)
#summary(PCA_Data)
PC1 <- PCA_Data$scores[,1]
PC2 <- PCA_Data$scores[,2]
X <- cbind(PC1, PC2)
km <- kmeans(X, centers = 7)
plot(PC1, PC2, col = km$cluster, xlab = "PC1", ylab = "PC2", main = "K-means clustering with PCA")
points(km$centers, col = 1:7, pch = 3, cex = 2, lwd = 3)
# Chunk 17
OrderClust = function(clust)
{
o = order(clust$cluster)
kmclust = data.frame(clust$cluster[o])
return (kmclust)
}
# Chunk 18
d1 = OrderClust(km)
d2 = OrderClust(pam)
d3 = OrderClust(clara)
result = cbind(d1, d2, d3)
# Chunk 19
set.seed(123)
res1 = data %>%
scale() %>%
eclust("kmeans", k = 7, graph = TRUE)
res2 = data %>%
scale() %>%
eclust("pam", k = 7, graph = TRUE)
res3 = data %>%
scale() %>%
eclust("clara", k = 7, graph = TRUE)
s1 = fviz_silhouette(res1)
s2 = fviz_silhouette(res2)
s3 = fviz_silhouette(res3)
grid.arrange(s1, s2, s3, nrow = 2)
Dataset = function()
{
# n is the number of outliers to remove
df = read.csv("ListS&P500.csv", row.names = 1, sep = ';')
# Omit non available values if there are any left (pre-treated dataset)
df <- na.omit(df)
# Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
df <- scale(df)
return (df)
}
Dataset = function()
{
# n is the number of outliers to remove
df = read.csv("ListS&P500.csv", row.names = 1, sep = ';')
# Omit non available values if there are any left (pre-treated dataset)
df <- na.omit(df)
# Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
df <- scale(df)
return (df)
}
df = Dataset()
View(df)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(gridExtra)
library(fpc)
library(dbscan)
library(mclust)
library(adamethods)
library(car) # Companion to Applied Regression
library(caret)
library(randomForest)
library(mlbench)
library(fpc)
library(knitr) # package for building complex tables
library(kableExtra)
library(viridisLite) # color map package
# Chunk 3
Dataset = function()
{
# n is the number of outliers to remove
df = read.csv("ListS&P500.csv", row.names = 1, sep = ';')
# Omit non available values if there are any left (pre-treated dataset)
df <- na.omit(df)
# Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
df <- scale(df)
return (df)
}
# Chunk 4
df = Dataset()
# Chunk 5
# ensure the results are repeatable
#set.seed(7)
# calculate correlation matrix
#correlationMatrix <- cor(df)
# summarize the correlation matrix
#print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
#highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
#print(highlyCorrelated)
# Chunk 6
new.df = as.data.frame.matrix(df)
mod <- lm(new.df$Pre.Tax.ROE ~ ., data = new.df)
# Chunk 7
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]
#new.df[influential, ]
# Chunk 8
CompanyIndex = function(Df, vect)
{
indexes = 0
for (i in 1:length(vect))
{
indexes[i] = which(rownames(Df) == vect[i])
}
return (indexes)
}
# Chunk 9
outliers1 = CompanyIndex(Dataset(), influential)
outliers1
# Chunk 10
car_outliers = outlierTest(mod)
vect = names(car_outliers$rstudent)
outliers2 = CompanyIndex(df, vect)
car_outliers
outliers2
# Chunk 11
Outliers = function(Df, n, c)
{
#parameters : Df = dataframe, n = number of outliers, c = number of centers for clustering
PCA_Data = princomp(Df, cor = T)
PC1 = PCA_Data$scores[,1]
PC2 = PCA_Data$scores[,2]
X = cbind(PC1, PC2)
km = kmeans(X, centers = c)
centers = km$centers[km$cluster,]
distances = sqrt(rowSums((X-centers)^2))
outliers = order(distances, decreasing = T)[1:n]
plot(X, pch = 16, col = km$cluster, cex = 1, main = paste0("C = ", c, "     N = ", n))
points(km$centers, pch = 23, bg = "yellow", cex = 2, lwd = 2)
points(X[outliers,], pch = 25, col = "orange", cex = 2)
legend("topleft", legend = c("Cluster center", "Outliers"), pt.cex = 2, pch = c(23, 25), col = c("black", "orange"), pt.bg = c("yellow", NA))
return (outliers)
}
# Chunk 12
outliers3 = Outliers(df,30,1)
outliers3
# Chunk 13
outliers4 = do_knno(df, 10, 10)
outliers4
# Chunk 14
dataset = df[-outliers3,]
data = as.data.frame.matrix(dataset)
# Chunk 15
distance <- get_dist(dataset)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(dataset, centers = 2, nstart = 25)
p1 = fviz_cluster(k2, data = dataset) + ggtitle("k = 2")
k3 = kmeans(dataset, centers = 3, nstart = 25)
p2 = fviz_cluster(k3, data = dataset) + ggtitle("k = 3")
k4 = kmeans(dataset, centers = 4, nstart = 25)
p3 = fviz_cluster(k4, data = dataset) + ggtitle("k = 4")
grid.arrange(p1, p2, p3, nrow = 2)
# Chunk 16
set.seed(123)
fviz_nbclust(dataset, kmeans, method = "wss", k.max = 20)
fviz_nbclust(dataset, kmeans, method = "silhouette", k.max = 20)
fviz_nbclust(dataset, kmeans, method = "gap_stat", k.max = 20)
# Chunk 17
# Compute k-means
k = kmeans(dataset, centers = 7, nstart = 25)
a1 = fviz_cluster(k, data = dataset) + ggtitle("k-means")
# Compute PAM
pam <- pam(dataset, 7)
a2 = fviz_cluster(pam) + ggtitle("PAM")
# Compute CLARA
clara <- clara(dataset, 7, samples = 50, pamLike = TRUE)
a3 = fviz_cluster(clara) + ggtitle("CLARA")
grid.arrange(a1, a2, a3, nrow = 2)
PCA_Data = princomp(dataset, cor = T)
#str(PCA_Data)
#summary(PCA_Data)
PC1 <- PCA_Data$scores[,1]
PC2 <- PCA_Data$scores[,2]
X <- cbind(PC1, PC2)
km <- kmeans(X, centers = 7)
plot(PC1, PC2, col = km$cluster, xlab = "PC1", ylab = "PC2", main = "K-means clustering with PCA")
points(km$centers, col = 1:7, pch = 3, cex = 2, lwd = 3)
# Chunk 18
OrderClust = function(clust)
{
o = order(clust$cluster)
kmclust = data.frame(clust$cluster[o])
return (kmclust)
}
# Chunk 19
d1 = OrderClust(km)
d2 = OrderClust(pam)
d3 = OrderClust(clara)
result = cbind(d1, d2, d3)
# Chunk 20
set.seed(123)
res1 = data %>%
scale() %>%
eclust("kmeans", k = 7, graph = TRUE)
res2 = data %>%
scale() %>%
eclust("pam", k = 7, graph = TRUE)
res3 = data %>%
scale() %>%
eclust("clara", k = 7, graph = TRUE)
s1 = fviz_silhouette(res1)
s2 = fviz_silhouette(res2)
s3 = fviz_silhouette(res3)
grid.arrange(s1, s2, s3, nrow = 2)
# Chunk 21
Creer méthode :
inserer csv avec sector et trouver de quel secteur sont les entreprises du cluster
View(result)
