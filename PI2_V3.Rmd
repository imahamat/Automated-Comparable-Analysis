---
title: "PI2_V3"
subtitle: "Clustering"
author: Groupe 88
date: "`r format(Sys.time())`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: cerulean
    highlight: espresso
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Libraries
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(FactoMineR)
library(gridExtra)
library(fpc)
library(dbscan)
library(mclust)
library(adamethods)
library(car) # Companion to Applied Regression
library(caret)
library(randomForest)
library(mlbench)
library(fpc)
library(knitr) # package for building complex tables 
library(kableExtra)
library(viridisLite) # color map package
```


# Dataset importation
```{r}
setwd('/Users/imahamat/Documents/IBO/Projets/pi2/Automated-Comparable-Analysis')
Dataset = function(file)
{
  # n is the number of outliers to remove
  df = read.csv(file, row.names = 1, sep = ';')
  # Omit non available values if there are any left (pre-treated dataset)
  df <- na.omit(df)
  
  # Feature scaling through standardization is an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the    features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It helps to normalise the      data within a particular range and it can also help in speeding up the calculations in the algorithms.
  df <- scale(df)
  
  return (df)
}
```



```{r}
df = Dataset("ListS&P500.csv")
df2 = Dataset("ListS&P500_Sector.csv")
Sector = df2$SECTOR
Sector
Company = df2$Company
Company
head(df)
head(df2)
```

Remove Redundant Features
Generally, you want to remove attributes with an absolute correlation of 0.75 or higher.
```{r}
# ensure the results are repeatable
#set.seed(7)
# calculate correlation matrix
#correlationMatrix <- cor(df)
# summarize the correlation matrix
#print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
#highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
#print(highlyCorrelated)
```


# Outlier Treatment

Outliers can drastically bias/change the fit estimates and predictions. 


## Detect outliers

Multivariate Model Approach
```{r}
new.df = as.data.frame.matrix(df)

mod <- lm(new.df$ROE ~ ., data = new.df)
```

## Cook's distance outlier test

Declaring an observation as an outlier based on just one feature could lead to unrealistic inferences. When you have to decide if an individual entity is an extreme value or not, it is better to collectively consider the features (X's) that matter -> Using Cook's Distance.

Cook's distance is a measure computed with respect to a given regression model and therefore is impacted only by the X variables included in the model. But, what does cook's distance mean? It computes the influence exerted by each data point (row) on the predicted outcome.

The cook's distance for each observation i measures the change in Y^ (fitted Y) for all observations with and without the presence of observation i, so we know how much the observation i impacted the fitted values. Mathematically, cook's distance Di for observation i is computed as:

Di=???nj=1(Y^j???Y^j(i))^2/p*MSE

where : 

Y^j is the value of jth fitted response when all the observations are included.
Y^j(i) is the value of jth fitted response, where the fit does not include observation i.
MSE is the mean squared error.
p is the number of coefficients in the regression model.

In general use, the observations that have a cook's distance greater than 4 times the mean may be classified as influential.
```{r}
cooksd <- cooks.distance(mod)

plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels

influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]
#new.df[influential, ]
```

Get company indexes (vector of indexes)
```{r}
CompanyIndex = function(Df, vect)
{
  indexes = 0
  
  for (i in 1:length(vect))
  {
    indexes[i] = which(rownames(Df) == vect[i])
  }
  
  return (indexes)
}
```

```{r}
outliers1 = CompanyIndex(df, influential)
outliers1
```


## Companion to Applied Regression outlier test

The function outlierTest from car package (Companion to Applied Regression) gives the most extreme observation based on the given model.
```{r}
car_outliers = outlierTest(mod)

vect = names(car_outliers$rstudent)

outliers2 = CompanyIndex(df, vect)

car_outliers
outliers2
```


## Euclidean distance outlier test

The function takes for parameters a dataframe, the number of outliers to detect and the number of centers for the k-means clustering.

First, it will do a PCA :
-statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.
-consiste ? transformer des variables corr?l?es en nouvelles variables d?corr?l?es les unes des autres. Ces nouvelles variables sont nomm?es ? composantes principales ?, ou axes principaux. Elle permet de r?duire le nombre de variables et de rendre l'information moins redondante.

In the next step we will calculate the distance (Euclidean) between the objects and cluster centers to determine the outliers and identify n largest distances which are outliers. 

Finally we plot the graph with c clusters et n outliers detected we can remove later on for our final clustering.

Get outliers (vector of n indexes of the outliers):
```{r}
Outliers = function(Df, n, c)
{
  #parameters : Df = dataframe, n = number of outliers, c = number of centers for clustering
  PCA_Data = princomp(Df, cor = T)
  PC1 = PCA_Data$scores[,1]
  PC2 = PCA_Data$scores[,2]

  X = cbind(PC1, PC2)
  km = kmeans(X, centers = c)
  
  centers = km$centers[km$cluster,]
  distances = sqrt(rowSums((X-centers)^2))
  outliers = order(distances, decreasing = T)[1:n]

  plot(X, pch = 16, col = km$cluster, cex = 1, main = paste0("C = ", c, "     N = ", n))
  points(km$centers, pch = 23, bg = "yellow", cex = 2, lwd = 2)
  points(X[outliers,], pch = 25, col = "orange", cex = 2)
  legend("topleft", legend = c("Cluster center", "Outliers"), pt.cex = 2, pch = c(23, 25), col = c("black", "orange"), pt.bg = c("yellow", NA))
  
  return (outliers)
}
```

```{r}
outliers3 = Outliers(df,30,1)
outliers3
```


## kNN outlier test

Description:
k-nearest neighbors outlier detection method (kNNo). Each point's anomaly score is the distance to its kth nearest neighbor in the data set. Then, all points are ranked based on this distance. The higher an example's score is, the more anomalous it is.

Usage:
do_knno(data, k, top_n)

Arguments:
data    Data observations.
k       Number of neighbors of a point that we are interested in.
top_n	  Total number of outliers we are interested in.
```{r}
outliers4 = do_knno(df, 10, 10)
outliers4
```


We can now remove the outliers from de dataset and begin the clustering methods.
```{r}
dataset = df[-outliers3,]
data = as.data.frame.matrix(dataset)
```


# Clustering

## Preview

Preview with 2, 3 and 4 centers to get some insights.
```{r}
distance <- get_dist(dataset)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

k2 <- kmeans(dataset, centers = 2, nstart = 25)
p1 = fviz_cluster(k2, data = dataset) + ggtitle("k = 2")

k3 = kmeans(dataset, centers = 3, nstart = 25)
p2 = fviz_cluster(k3, data = dataset) + ggtitle("k = 3")

k4 = kmeans(dataset, centers = 4, nstart = 25)
p3 = fviz_cluster(k4, data = dataset) + ggtitle("k = 4")

grid.arrange(p1, p2, p3, nrow = 2)
```

## Optimal number of clusters
```{r}
set.seed(123)

fviz_nbclust(dataset, kmeans, method = "wss", k.max = 20)

fviz_nbclust(dataset, kmeans, method = "silhouette", k.max = 20)

fviz_nbclust(dataset, kmeans, method = "gap_stat", k.max = 20)
```

## Clustering methods
```{r}
# Compute k-means
k = kmeans(dataset, centers = 7, nstart = 25)
a1 = fviz_cluster(k, data = dataset) + ggtitle("k-means")

# Compute PAM
pam <- pam(dataset, 7) 
a2 = fviz_cluster(pam) + ggtitle("PAM")

# Compute CLARA
clara <- clara(dataset, 7, samples = 50, pamLike = TRUE)
a3 = fviz_cluster(clara) + ggtitle("CLARA")

grid.arrange(a1, a2, a3, nrow = 2)


PCA_Data = princomp(dataset, cor = T)

#str(PCA_Data)
#summary(PCA_Data)

PC1 <- PCA_Data$scores[,1]
PC2 <- PCA_Data$scores[,2]

X <- cbind(PC1, PC2)
km <- kmeans(X, centers = 7)
plot(PC1, PC2, col = km$cluster, xlab = "PC1", ylab = "PC2", main = "K-means clustering with PCA")
points(km$centers, col = 1:7, pch = 3, cex = 2, lwd = 3)
```

```{r}
OrderClust = function(clust)
{
  
  o = order(clust$cluster)
  kmclust = data.frame("Cluster" = c(clust$cluster[o]))
  return (kmclust)
}
```

```{r}
d1 = OrderClust(km)
d1
#write.csv(d1,file = "km2.csv",row.names = FALSE)
d2 = OrderClust(pam)
d3 = OrderClust(clara)

result = cbind(d1, d2, d3)
```


## Clustering test
```{r}
set.seed(123)

res1 = data %>%
scale() %>%
eclust("kmeans", k = 7, graph = TRUE)

res2 = data %>%
scale() %>%
eclust("pam", k = 7, graph = TRUE)

res3 = data %>%
scale() %>%
eclust("clara", k = 7, graph = TRUE)

s1 = fviz_silhouette(res1)
s2 = fviz_silhouette(res2)
s3 = fviz_silhouette(res3)

grid.arrange(s1, s2, s3, nrow = 2)
```


```{r}
#Creer m√©thode :
# inserer csv avec sector et trouver de quel secteur sont les entreprises du cluster
km_c = read.csv("km.csv", row.names = 1, sep = ';')
km_c$

movie_matrix <- matrix(km_cluster, nrow = 2, ncol = 3)
movie_matrix

pam_cluster = OrderClust(pam)
pam_cluster

clara_cluster = OrderClust(clara)
compare = data.frame(Company, Sector,kmclust)
compare

```

